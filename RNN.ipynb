{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX_u7ygsEkml"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('reddit-comments-2015-08.csv',nrows=500)"
      ],
      "metadata": {
        "id": "-Aq9IlOYKPIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body = data['body'].values\n",
        "\n",
        "sentences = [sent_tokenize(text) for text in body]\n",
        "\n",
        "wrds = []\n",
        "for sentence in sentences:\n",
        "    tokenized_sentence = []\n",
        "    for word in sentence:\n",
        "        if isinstance(word, str) and word.strip():\n",
        "            tokenized_sentence.extend(word_tokenize(word))\n",
        "    wrds.append(tokenized_sentence)"
      ],
      "metadata": {
        "id": "VijnpwRsExZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwrd = set(stopwords.wrds('english'))\n",
        "\n",
        "cleanwrd = []\n",
        "for sentence in wrds:\n",
        "    filtered_sentence = [word for word in sentence if word.lower() not in stopwrd]\n",
        "    cleanwrd.append(filtered_sentence)\n",
        "\n",
        "all_wrds = [word for sentence in cleanwrd for word in sentence]\n",
        "word_counts = Counter(all_wrds)"
      ],
      "metadata": {
        "id": "hpluLoY9GB8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I_threshhold = 5\n",
        "infrequent_wrds = [word for word, count in word_counts.items() if count < I_threshhold]\n",
        "\n",
        "cleanwrd = [[word for word in sentence if word not in infrequent_wrds] for sentence in cleanwrd]\n",
        "\n",
        "\n",
        "data['cleanwrd'] = cleanwrd\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HL4A9gVgGdSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dim = 100\n",
        "hidden_dim = 200\n",
        "\n",
        "\n",
        "output_dim = 2\n",
        "\n",
        "bptt_truncate = 8\n",
        "\n",
        "W = np.random.uniform(-np.sqrt(1.0/hidden_dim), np.sqrt(1.0/hidden_dim), (hidden_dim, hidden_dim))\n",
        "\n",
        "U = np.random.uniform(-np.sqrt(1.0/word_dim), np.sqrt(1.0/word_dim), (hidden_dim, word_dim))\n",
        "\n",
        "V = np.random.uniform(-np.sqrt(1.0/hidden_dim), np.sqrt(1.0/hidden_dim), (output_dim, hidden_dim))\n"
      ],
      "metadata": {
        "id": "S6hwqC6vGnuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "esUUZnbIGpzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fwd_propogation(x):\n",
        "    len = len(x)\n",
        "\n",
        "    s = np.zeros((len + 1, hidden_dim))\n",
        "\n",
        "    s[-1] = np.zeros(hidden_dim)\n",
        "\n",
        "    a = np.zeros((len, output_dim))\n",
        "\n",
        "\n",
        "    for t in range(len):\n",
        "\n",
        "        s[len] = sigmoid(U[:, x[t]] + W.dot(s[t-1]))\n",
        "\n",
        "        a[len] = sigmoid(V.dot(s[t]))\n",
        "\n",
        "\n",
        "    return a, s"
      ],
      "metadata": {
        "id": "F1vgMhTOGtJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(y, a):\n",
        "  x = -np.mean(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
        "  return x"
      ],
      "metadata": {
        "id": "gjdsJbkhJc0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = {word: idx for idx, word in enumerate(set(word for sentence in cleanwrd for word in sentence))}\n",
        "\n",
        "vocabsize = len(word_index)\n",
        "\n",
        "word_dim = 100\n",
        "\n",
        "\n",
        "x_train = [[min(word_index[word], word_dim - 1) for word in sentence] for sentence in cleanwrd]\n",
        "\n",
        "y_train = np.random.randint(2, size=(len(x_train), output_dim))\n",
        "\n",
        "o_train, _ = fwd_propogation(x_train[0])\n",
        "\n",
        "\n",
        "\n",
        "loss = calculate_loss(y_train[0], o_train)\n",
        "\n",
        "print(\"Loss:\")\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alOzNBLgJenj",
        "outputId": "fedb34af-7685-4f03-ee52-065c6b2da70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.5667208316106862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19myjLTBWeZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}